# ğŸ—„ï¸ LLM-Vector-database

"Don't fine-tune your LLM, **Construct** a vector database."

"There is **No Hallucination** here."

## ğŸ” Overview

LLM-Vector-database is a powerful tool that allows you to construct a vector database using sentence embeddings. Instead of fine-tuning your Large Language Model (LLM), this project provides a unique approach to natural language processing and understanding. By embedding sentences into a vector space and constructing a database from these vectors, you can generate text responses based on this database. This makes it an ideal resource for chatbot development and other natural language processing applications.

<img src="https://github.com/juicyjung/LLM-Vector-database/assets/83687471/384a9fe0-00dc-454a-a625-9fa5c22bad11">
<br>
<p align="center">
  <img src="https://github.com/juicyjung/LLM-Vector-database/assets/83687471/66aa6397-38c4-4d49-a298-4a736e102111" width="500">
</p>


## ğŸŒŸ Features

- Sentence embedding: Convert your sentences into vector representations.
- Vector database construction: Build a database from your sentence vectors.
- Text generation: Generate text responses based on the vector database.

## ğŸš€ Quickstart Guide

Follow these steps to get started with the LLM-Vector-database:

**1. Clone the repository** 

Use the following command to clone the repository:

```bash
git clone https://github.com/juicyjung/LLM-Vector-database.git
```

**2. Install the necessary dependencies**

After cloning the repository, navigate into the directory and install the necessary dependencies, vectordb and torch(appropriate version for your environment) by executing:

```bash
pip install poetry
poetry install
```

That's it! You've successfully set up LLM-Vector-database on your machine.

## ğŸ’» Usage

Follow these steps to utilize LLM-Vector-database in your project:

```python
from llmvdb import Llmvdb
from llmvdb.embedding.model import HuggingFaceEmbedding
from llmvdb.llm.openai import OpenAI

embedding = HuggingFaceEmbedding()
llm = OpenAI(instruction="ë„ˆëŠ” ë²•ë¥  ìë¬¸ì„ ìœ„í•œ ì±—ë´‡ì´ì•¼. ì‚¬ìš©ìë¥¼ ìœ„í•´ ë¨¼ì € ê°ì •ì ì¸ ê³µê°ì„ í•´ì¤˜ì•¼í•´.")

your_llm = Llmvdb(
    embedding,
    llm,
    hugging_face="juicyjung/easylaw_kr_documents",
    workspace="workspace_path",
)

answer = your_llm.generate_prompt("ì›”ì„¸ë°©ì„ ì–»ì–´ ìì·¨ë¥¼ í•˜ê³  ìˆëŠ”ë° êµ°ëŒ€ì— ê°€ì•¼í•©ë‹ˆë‹¤. ë³´ì¦ê¸ˆì„ ëŒë ¤ë°›ì„ ìˆ˜ ìˆì„ê¹Œìš”?")
print(answer)
```

The above code will return the following:
```
êµ°ëŒ€ì— ì…ëŒ€í•´ì•¼ í•˜ëŠ” ê²½ìš°ì—ëŠ” ì„ì°¨ì¸ì´ ì„ëŒ€ì°¨ ê³„ì•½ì„ ì¤‘ë„í•´ì§€í•  ìˆ˜ ìˆëŠ” ì‚¬ìœ ì— í•´ë‹¹í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë”°ë¼ì„œ, ì•½ì •í•œ ê¸°ê°„ì´ ë‚¨ì€ ì„ëŒ€ì°¨ì˜ ê²½ìš°ì—ëŠ” ë³´ì¦ê¸ˆì„ ëŒë ¤ë°›ì„ ìˆ˜ ì—†ìœ¼ë©°, ì•½ì •í•œ ê¸°ê°„ ë™ì•ˆ ì›”ì„¸ë¥¼ ì§€
ê¸‰í•´ì•¼ í•©ë‹ˆë‹¤.
```

## ğŸ† Advantages of Using LLM-Vector-database

Using the method described above, we were able to significantly address two major issues that arose when fine-tuning LAW-Alpaca.

### 1. Reduced Training Burden

Fine-tuning requires high-performance GPU resources and takes about 5 hours each time based on approximately 2000 data. However, by using a Vector Database, we were able to use the LLM off-the-shelf, which saved costs during training. The process of embedding and constructing the vector database took less than 1 minute, significantly reducing the time and resources required compared to traditional fine-tuning methods.

   
### 2. Solved Hallucination Problem

The advantage of a language model is inference and generation from given language data, not fact searching. Therefore, if you simply ask the LLM a fact-based question, it can produce plausible but false information, regardless of how much fine-tuning has been done. However, when we applied this architecture, we changed the role of the LLM from fact-based questioning to a QA task, preserving the LLM's strength in inference while solving the problem of hallucination.


## ğŸ¤ Contributing

Contributions are welcome! Please check out the todos below, and feel free to open a pull request.

## ğŸ“ Contact

If you have any questions, feel free to reach out to us. We'd be more than happy to assist you!
